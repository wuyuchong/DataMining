---
documentclass: ctexart
output:
  rticles::ctex:
    fig_caption: yes
    number_sections: yes
    toc: yes
    template: template.tex
    highlight: espresso
classoption: "hyperref,"
geometry: margin=1in
csl: chinese-gb7714-2005-numeric.csl
header-includes:
   - \usepackage{graphicx}
   - \usepackage{float}
logo: "cufe.jpg"
---

\newpage

```{r eval=FALSE, include=FALSE}
library(knitr)
library(reticulate)
library(magrittr)
use_python("/usr/local/bin/python3")
py_config()
```

# 简介

此文档为两个算法的代码文档，包括了**Logisitic 回归**和**神经网络**两个模型算法。

1. model 文件夹：存放两个算法模型的类
2. source 文件夹：存放依赖函数
3. main 文件：测试用例 demo

# Logistic 回归算法

我们使用**梯度下降**的优化方法构建 logit 模型。

## 模型求解步骤

如果p是一个事件的概率，这个事件的发生比率就是p/(1-p)。逻辑回归就是建立模型预测这一比率的对数:

$$\log \left(\frac{p}{1-p}\right)=\beta_{0}+\beta_{1} x_{1}+\cdots+\beta_{P} x_{P}$$

即：

$$p=\frac{1}{1+\exp \left[-\left(\beta_{0}+\beta_{1} x_{1}+\cdots+\beta_{P} x_{P}\right)\right]}$$

假设我们有n个独立的训练样本{(x1, y1) ,(x2, y2),…, (xn, yn)}，y={0, 1}。那每一个观察到的样本(xi, yi)出现的概率是：

$$P\left(\mathrm{y}_{i}, \mathrm{x}_{i}\right)=P\left(\mathrm{y}_{i}=1 | \mathrm{x}_{i}\right)^{y_{i}}\left(1-P\left(\mathrm{y}_{i}=1 | \mathrm{x}_{i}\right)\right)^{1-y_{i}}$$

那我们的整个样本集，也就是n个独立的样本出现的似然函数为:

$$L(\theta)=\prod P\left(\mathrm{y}_{i}=1 | \mathrm{x}_{i}\right)^{y_{i}}\left(1-P\left(\mathrm{y}_{i}=1 | \mathrm{x}_{i}\right)\right)^{1-y_{i}}$$

那么，损失函数（cost function）就是最大似然函数**取对数**。 ^[最大似然法就是求模型中使得似然函数最大的系数取值*]

用$L(\theta)$对$\theta$求导，得到：

$$\frac{\partial L(\theta)}{\partial \theta}=\sum_{i=1}^{n} y_{i} x_{i}-\sum_{i}^{n} \frac{e^{\theta^{T} x_{i}}}{1+e^{\theta_{x_{i}}}} x_{i}=\sum_{i=1}^{n}\left(y_{i}-\sigma\left(\theta^{T} x_{i}\right)\right) x_{i}$$

令该导数为0，无法解析求解。使用梯度下降法 @汪宝彬2011随机梯度下降法的一些性质 ，那么：

$$\theta^{t+1}=\theta^{t}-\alpha \frac{\partial L(\theta)}{\partial \theta}=\theta^{t}-\alpha \sum_{i=1}^{n}\left(y_{i}-\sigma\left(\theta^{T} x_{i}\right)\right) x_{j}$$

在进行训练之前，我们对各个变量的数据进行标准化处理。然后，我们让学习率逐步递减进行训练。 @LiFeng

## 代码实现 - 逐步讲解（Step by Step）

### 包和数据导入

```{python}
import numpy as np
import random
import math
from sklearn import datasets
```

我们读取经典的 iris 数据集。 ^[只展示前5行]

```{python}
iris = datasets.load_iris()
X = iris['data']
y = iris['target']
X = X[y!=2]
y = y[y!=2]
X[1:5]
y[1:5]
```

### 定义sigmoid函数

```{r echo=FALSE}
## 画sigmoid函数
sigmoid = function(x)
{
  outcome = 1/(1+exp(-x))
  return(outcome)
}
```

```{r echo=FALSE, fig.align='center', fig.cap='sigmoid function', out.width='40%'}
curve(1/(1+exp(-x)), from = -6, to = 6, n = 100, add = FALSE, type = "l", xlab = 'x', ylab = 'y')
```

sigmoid 函数

```{python}
def sigmoid(Xi,thetas):
    params = - np.sum(Xi * thetas)
    outcome = 1 /(1 + math.exp(params))
    return outcome
```

适用于矩阵的 sigmoid 函数

```{python}
def sigmoidMatrix(Xb,thetas):
    params = - Xb.dot(thetas)
    outcome = np.zeros(params.shape[0])
    for i in range(len(outcome)):
        outcome[i] = 1 /(1 + math.exp(params[i]))
    return outcome
```

带阈值判别的 sigmoid 函数

- 阈值（threshold）：用于给出概率后进行分类，默认为 50%

```{python}
def sigmoidThreshold(Xb,thetas):
    params = - Xb.dot(thetas)
    outcome = np.zeros(params.shape[0])
    for i in range(len(outcome)):
        outcome[i] = 1 /(1 + math.exp(params[i]))
        if outcome[i] >= 0.5:
            outcome[i] = 1
        else:
            outcome[i] = 0
    return outcome
```

### 定义损失函数

损失函数（cost function）就是最大似然函数**取对数**

```{python}
def costFunc(Xb,y):
    sum = 0.0
    for i in range(m):
        yPre = sigmoid(Xb[i,], thetas)
        if yPre == 1 or yPre == 0:
            return float(-2**31)
        sum += y[i] * math.log(yPre) + (1 - y[i]) * math.log(1-yPre)
    return -1/m * sum
```

### 用梯度下降法进行训练

初始化：

- 学习率（alpha）：用于调整每次迭代的对损失函数的影响大小
- 准确度（accuracy）：作为终止迭代的评判指标

```{python}
thetas = None
m = 0
alpha = 0.01
accuracy = 0.001
```


在第一列插入1，构成 Xb 矩阵

```{python}
thetas = np.full(X.shape[1]+1,0.5)
m = X.shape[0]
a = np.full((m, 1), 1)
Xb = np.column_stack((a, X))
n = X.shape[1] + 1
```

使用梯度下降法进行迭代：

```{python}
count = 1
while True:
    before = costFunc(Xb, y)
    c = sigmoidMatrix(Xb, thetas)-y
    for j in range(n):
        thetas[j] = thetas[j] -alpha * np.sum(c * Xb[:,j])
    after = costFunc(Xb, y)
    if after == before or math.fabs(after - before) < accuracy:
        print("迭代完成，损失函数值:",after)
        break
    print("迭代次数:",count)
    print("损失函数变化",(after - before))
    count += 1
```

迭代完成后得到的参数：

```{python}
thetas
```

### 预测

```{python}
a = np.full((len(X),1),1)
Xb = np.column_stack((a,X))
sigmoidThreshold(Xb, thetas)
```

## 代码实现 - 类封装

可以调整的参数包括：

- 学习率（alpha）：用于调整每次迭代的对损失函数的影响大小
- 准确度（accuracy）：作为终止迭代的评判指标
- 阈值（threshold）：用于给出概率后进行分类，默认为 50%

```{python}
# ----------------- 导入基本模块 -----------------
import numpy as np
import math

# ------------ 导入source中定义的函数 ------------
from source.sigmoid import sigmoid
from source.sigmoidMatrix import sigmoidMatrix
from source.sigmoidThreshold import sigmoidThreshold
    
# ----------------- 定义 base 类 -----------------
class Regression(object):
    def __init__(self, X, y, threshold = 0.5):
        self.thetas = None
        self.X = X
        self.y = y

# ---------------- 定义逻辑回归类 ----------------
class LogsiticRegression(Regression):
    def __init__(self, X, y, threshold = 0.5):
        Regression.__init__(self, X, y, threshold = 0.5) # 继承 Regression 类
        self.m = 0
        self.threshold = threshold
        self.epoch = 1
    
    def fit(self, alpha = 0.01, accuracy = 0.001):
        self.thetas = np.full(self.X.shape[1] + 1,0.5)
        self.m = self.X.shape[0]
        a = np.full((self.m, 1), 1)
        Xb = np.column_stack((a,self.X))
        n  = self.X.shape[1]+1

        while True:
            before = self.costFunc(Xb, y)
            c = sigmoidMatrix(Xb, self.thetas) - y
            for j in range(n):
                self.thetas[j] = self.thetas[j] - alpha * np.sum(c * Xb[:,j])
            after = self.costFunc(Xb, y)
            if after == before or math.fabs(after - before) < accuracy:
                break
            self.epoch += 1
 
    def costFunc(self, Xb, y):
        sum = 0.0
        for i in range(self.m):
            yPre = sigmoid(Xb[i,], self.thetas)
            if yPre == 1 or yPre == 0:
                return float(-2**31)
            sum += y[i] * math.log(yPre) + (1 - y[i]) * math.log(1 - yPre)
        return -1/self.m * sum
        
    def predict(self):
        a = np.full((len(X), 1), 1)
        Xb = np.column_stack((a, X))
        return sigmoidThreshold(Xb, self.thetas, self.threshold)
```

## 测试用例

我们使用经典的 iris 数据集作为测试用例。

```{python}
iris = datasets.load_iris()
X = iris['data']
y = iris['target']
X = X[y!=2]
y = y[y!=2]

Logstic = LogsiticRegression(X, y)    
Logstic.fit()
print("epoch:",Logstic.epoch)
print("theta:",Logstic.thetas)
y_predict = Logstic.predict()
y_predict
```

# 神经网络算法

我们使用**随机梯度下降**的优化方法构建一个较为简单的神经网络模型。 ^[考虑到神经网络是较为复杂的一类模型，在此我们只构造它的初始版本，限制较多，不具备广泛实用性。]

## 模型求解步骤

### 神经元

神经元是神经网络的基本单元。 @Chua1988Cellular 神经元先获得输入，然后执行某些数学运算后，再产生一个输出。 @庄镇泉1990神经网络与神经计算机:第二讲

对于一个二输入神经元，输出步骤为：
先将两个输入乘以权重，把两个结果想加，再加上一个偏置，最后将它们经过激活函数处理得到输出。 ^[神经元（Neurons）权重（weight）偏置（bias）激活函数（activation function）]

$$y = f(x1 × w1 + x2 × w2 + b)$$

我们选择 sigmoid 函数作为神经网络的激活函数。

$$S(x) = \frac{1}{1 + e^{-x}}$$

$$S'(x) = \frac{e^{-x}}{(1 + e^{-x})^2} = S(x)(1-S(x))$$

### 神经网络

考虑到单机运算速度的限制，我们搭建一个简单的神经网络。它具有2个输入、一个包含2个神经元的隐藏层（h1和h2）、包含1个神经元的输出层（o1）。 
^[A neural network with:
        - 2 inputs
        - a hidden layer with 2 neurons (h1, h2)
        - an output layer with 1 neuron (o1)]
 @阎平凡2005人工神经网络与模拟进化计算

### 前馈

把神经元的输入向前传递获得输出的过程称为前馈 ^[前馈（feedforward）]

### 损失

在训练神经网络之前，我们需要有一个标准定义，以便我们进行改进。我们采用均方误差（MSE）来定义损失（loss）：

$$ \mathrm{MSE} = \frac{1}{n} \sum_{i=1}^n (y_{true} - y_{pred}) $$

我们在训练中每次只取一个样本，那么损失函数为：

$$ \begin{aligned} \mathrm{MSE} &=\frac{1}{1} \sum_{i=1}^{1}\left(y_{t r u e}-y_{p r e d}\right)^{2} \\ &=\left(y_{t r u e}-y_{p r e d}\right)^{2} \\ &=\left(1-y_{p r e d}\right)^{2} \end{aligned} $$

### 训练神经网络

训练神经网络就是将损失最小化，预测结果越好，损失就越低。


由于预测值是由一系列网络权重和偏置计算出来的，所以损失函数实际上是包含多个权重、偏置的多元函数：

$$L\left(w_{1}, w_{2}, w_{3}, w_{4}, w_{5}, w_{6}, b_{1}, b_{2}, b_{3}\right)$$
由链式求导法则（以w1为例）：

$$\frac{\partial L}{\partial w_{1}}=\frac{\partial L}{\partial y_{p r e d}} * \frac{\partial y_{p r e d}}{\partial h_{1}} * \frac{\partial h_{1}}{\partial w_{1}}$$

这种向后计算偏导数的系统称为反向传导。 ^[反向传导（backpropagation）]

### SGD优化方法

我们使用随机梯度下降（SGD）的优化算法 @王功鹏2018基于卷积神经网络的随机梯度下降算法 来逐步改变网络的权重w和偏置b，使损失函数会缓慢降低，从而改进我们的神经网络。以w1为例：

$$w_{1} \leftarrow w_{1}-\eta \frac{\partial L}{\partial w_{1}}$$



```{r echo=FALSE, fig.align='center', fig.cap='sigmoid function', out.width='40%'}
curve(1/(1+exp(-x)), from = -6, to = 6, n = 100, add = FALSE, type = "l", xlab = 'x', ylab = 'y')
```

```{r echo=FALSE}
### 定义 sigmoid 的导函数

deriv_sigmoid = function(x)
{
  fx = sigmoid(x)
  return(fx * (1 - fx))
}
```

```{r echo=FALSE}
### 定义损失函数

mse_loss = function(y_true, y_pred)
{
  return(mean((y_true - y_pred)^2))
}
```


```{r echo=FALSE, fig.align='center', fig.cap='搭建的神经网络示意图', out.width='80%'}
knitr::include_graphics("搭建的神经网络示意图.png")
```

### 权重的初始化

神经网络中结点的各权重(weight)和偏置(bias) 的初始化均服从标准正态分布

$$ Weight_i,Bias_i \sim N(0,1)$$


```{r echo=FALSE}
# --- 权重
w1 = rnorm(1)
w2 = rnorm(1)
w3 = rnorm(1)
w4 = rnorm(1)
w5 = rnorm(1)
w6 = rnorm(1)

# --- 截距
b1 = rnorm(1)
b2 = rnorm(1)
b3 = rnorm(1)
```

```{r echo=FALSE}
feedforward = function(x, w1, w2, w3, w4, w5, w6, b1, b2, b3)
{
  h1 = sigmoid(w1 * x[1] + w2 * x[2] + b1)
  h2 = sigmoid(w3 * x[1] + w4 * x[2] + b2)
  o1 = sigmoid(w5 * h1 + w6 * h2 + b3)
  return(o1)
}
```

## 模型结果

从结果上看，随着迭代次数的增加，均方误差先快速减小，后趋向于稳定。


# 参考文献








