@article{庄镇泉1990神经网络与神经计算机:第二讲,
  title={神经网络与神经计算机:第二讲 神经网络的学习算法},
  author={庄镇泉 and 王东生},
  journal={电子技术应用},
  number={5},
  pages={38-41},
  year={1990},
 keywords={神经网络理论;感知机学习算法;神经计算机;神经元;多层网络;学习法则;学习功能;模拟退火算法;波尔兹曼机;权值},
 abstract={正学习功能是神经网络的主要特征之一。各种学习算法的研究,在神经网络理论发展过程中,起着重要的作用。从40年代Hebb提出的学习法则,到60年代的感知机学习算法,以及以后的多层网络学习算法和近},
}

@book{阎平凡2005人工神经网络与模拟进化计算,
  title={人工神经网络与模拟进化计算(第2版)},
  author={阎平凡},
  year={2005},
 abstract={本书较系统全面地讨论了人工神经网络与模拟进化计算的理论和工程应用，特别在学习理论和网络结构选择、动态神经网络、贝叶斯方法的应用以及模拟进化计算中一些理论问题等方面的论述更为系统深入。讲解中力求讲清物理概念，以便读者深入理解一些主要方法的思路。第二版加强了关于统计学习理论、核方法与支持向量机、自组织网络的灵活应用、盲信号处理等方面的内容，增加了神经网络在生物信息学和金融方面应用的实例，以及最近的参考文献，以便反映了这一领域的新进展。为了便于掌握主要内容，对章节顺序也做了调整。模拟进化计算部分增加了分布估计算法一章。},
}

@article{王功鹏2018基于卷积神经网络的随机梯度下降算法,
  title={基于卷积神经网络的随机梯度下降算法},
  author={王功鹏 and 段萌 and 牛常勇},
  journal={计算机工程与设计},
  volume={39},
  number={2},
  pages={441-445},
  year={2018},
 keywords={卷积神经网络;随机梯度下降算法;自适应学习率更新算法;LeakyRelu激活函数;快速收敛},
 abstract={为解决卷积神经网络(CNN)中随机梯度下降算法(SGD)的学习率设置不当对SGD算法的影响,提出一种学习率自适应SGD的更新算法,随着迭代的进行该算法使学习率呈现周期性的改变。针对CNN中Relu激活函数将CNN中的阈值为负的神经元丢弃的缺陷,设计选择LeakyRelu作为激活函数的CNN。实验验证了使用该激活函数的有效性,实验结果表明,采用上述学习率更新算法的SGD可以使网络快速收敛,提高了学习正确率;通过将LeakyRelu激活函数和采用上述学习率更新算法的SGD相结合,进一步提高CNN的学习正确率。},
}

@article{汪宝彬2011随机梯度下降法的一些性质,
  title={随机梯度下降法的一些性质},
  author={汪宝彬 and 汪玉霞},
  journal={数学杂志},
  volume={31},
  number={6},
  pages={1041-1044},
  year={2011},
 keywords={梯度下降法;核空间;随机逼近;逼近能力},
 abstract={本文研究了一般核空间下的随机梯度下降法.通过迭代方法,给出了该算法的一些重要性质,这些性质对于研究收敛速度起到至关重要的作用.},
}

@inproceedings{Chua1988Cellular,
  title={Cellular neural networks:theory},
  author={Chua, L. O. and Yang, L.},
  booktitle={IEEE International Workshop on Cellular Neural Networks & Their Applications},
  year={1988},
 keywords={analogue computer circuits;cellular arrays;computerised signal processing;neural nets;parallel architectures;real-time systems;VLSI implementation;active networks;cellular neural networks;continuous-time feature},
 abstract={A novel class of information-processing systems called cellular neural networks is proposed. Like neural networks, they are large-scale nonlinear analog circuits that process signals in real time. Like cellular automata, they consist of a massive aggregate of regularly spaced circuit clones, called cells, which communicate with each other directly only through their nearest neighbors. Each cell is made of a linear capacitor, a nonlinear voltage-controlled current source, and a few resistive linear circuit elements. Cellular neural networks share the best features of both worlds: their continuous-time feature allows real-time signal processing, and their local interconnection feature makes them particularly adapted for VLSI implementation. Cellular neural networks are uniquely suited for high-speed parallel signal processing.<<ETX xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">>},
}

@online{LiFeng,
howpublished = {\url{https://feng.li/teaching/statcomp/}},
note = {Accessed July 13, 2019},
title = {统计计算课程},
author = {李丰}
}

@online{WebScraping,
howpublished = {\url{https://feng.li/files/statcomp/SC-L13-WebScraping/index.html}},
note = {Accessed July 13, 2019},
title = {网络数据抓取},
author = {李丰}
}

@online{neuralnetwork,
howpublished = {\url{https://victorzhou.com/blog/intro-to-neural-networks/}},
note = {Accessed May 23, 2020},
title = {Machine Learning for Beginners: An Introduction to Neural Networks},
author = {Victor Zhou}
}
